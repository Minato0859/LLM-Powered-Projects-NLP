{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98FpJ6zEGIWq"
      },
      "source": [
        "## Natural Language Processing Pipelines (NLP Pipelines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSXKMATMGNbx"
      },
      "source": [
        "NLP algorithms are based on machine learning algorithms. Doing anything complicated in machine learning usually means building a pipeline. The idea is to break up your problem into very small pieces and then use machine learning to solve each smaller piece separately. Then by chaining together several machine learning models that feed into each other, you can do very complicated things."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bLKvuD6GomW"
      },
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=16e6wwg2eKxwwZgQ2DOfY65SEtpdX8Kv-)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, you’ll learn:\n",
        "\n",
        "    How to apply pre-processing techniques\n",
        "    How to apply text-normalization techniques\n",
        "    How to use spaCy and nltk\n"
      ],
      "metadata": {
        "id": "dbpn8BQg_l2a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfPQO1QtiLYo",
        "outputId": "6e71aecd-3950-4876-c282-83efc57b0967"
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apc0Sigvt5SN",
        "outputId": "a22f546c-a4f1-40c1-cd05-9c14acbd1bc5"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 37.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gMyOcKevKlm",
        "outputId": "1856500d-f451-490d-a7ca-fabad4f3f1d8"
      },
      "source": [
        "!pip install word2number"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=d12dc8a340e80ac0cbe7a7e15604a34a4d7209fb87bf701a6556a2efa454fb97\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jan_f3OUL8fE"
      },
      "source": [
        "## I. Text Processing in Python\n",
        "\n",
        "For text processing in Python, two popular libraries, namely NLTK (Natural Language Toolkit) and spaCy will be used in the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3W3pcgAqseb"
      },
      "source": [
        "For text processing we can perform a series of steps:\n",
        "\n",
        "1.  Remove symbols\n",
        "2. Remove non-ASCII characters\n",
        "3.    Remove extra whitespaces\n",
        "5.    Expand contractions\n",
        "6. Treatment for numbers\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"I've used Natural Language proceessing \"NLP\" \"\"\""
      ],
      "metadata": {
        "id": "gBQQLwf6dqKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JH5N7ysG4OC"
      },
      "source": [
        "text = \"Natural language processing (NLP) refers to the branch of @computer science—and more specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBjLgFbxx8Pb"
      },
      "source": [
        "##Remove symbols\n",
        "\n",
        "A text may contain some unwanted symbols which will be a noise for our text analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "EXqVO08gyRky",
        "outputId": "f4c3c481-09bb-4a15-b14a-ca25a49cd7d6"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def remove_symbols(text):\n",
        "    symbols = \"'\\<>?;:#@&()—\"\n",
        "    for i in range(len(symbols)):\n",
        "        text = np.char.replace(text, symbols[i], '')\n",
        "    return str(text)\n",
        "\n",
        "\n",
        "remove_symbols(\"Do we have extra #symbols in this @sentence. \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Do we have extra symbols in this sentence. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remove_symbols(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "s9260rm2hBLA",
        "outputId": "6c9dfeee-d60a-4bdf-a1d3-b0bf2a001c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Natural language processing NLP refers to the branch of computer scienceand more specifically, the branch of artificial intelligence or AIconcerned with giving computers the ability to understand text and spoken words in much the same way human beings can.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNEdOCca-KMa"
      },
      "source": [
        "##Removing Non-ASCII characters\n",
        "\n",
        "ASCII represents lowercase letters (a-z), uppercase letters (A-Z), digits (0-9) and symbols such as punctuation marks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CHDbgM4W-Ngy",
        "outputId": "0fbe06c6-8042-4bcb-cf97-766d98d42837"
      },
      "source": [
        "def remove_non_ascii(text):\n",
        "  # encoding the text to ASCII format\n",
        "  text_encode = text.encode(encoding=\"ascii\", errors=\"ignore\")\n",
        "  # decoding the text\n",
        "  text_decode = text_encode.decode()\n",
        "  return text_decode\n",
        "\n",
        "remove_non_ascii(\"Python is easy \\u200c to learn\" )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Python is easy  to learn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ny7mMZG1jksQ",
        "outputId": "b34e589c-63f4-4bb6-fc86-04b0edb9b40d"
      },
      "source": [
        "remove_non_ascii(\"àa string withé fuünny charactersß.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a string with funny characters.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the case of removing all the symbols and non-ASCII characters, one way is keeping only the characters and numbers in the text."
      ],
      "metadata": {
        "id": "8dj3-C0uZx8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #natual language toolkit\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re #regular expressions\n",
        "\n",
        "def keep_characters_numbers(text):\n",
        "  # replace any text that is not characters and numbers\n",
        "  cleaned_text = []\n",
        "  for word in word_tokenize(text):\n",
        "    cleaned_text.append(re.sub(\"[^a-zA-Z0-9]\", \"\", word))\n",
        "  return \" \".join(cleaned_text)"
      ],
      "metadata": {
        "id": "dPFWiRIxZwhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb79e3e3-ca8d-408b-9832-d655bc006533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keep_characters_numbers(\"àa string withé fuünny charactersß. The number is 2012.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "F6SkF7PDa5Tm",
        "outputId": "54266a6e-ece0-4aa2-9d6e-e0a96e69a2b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a string with funny characters  The number is 2012 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0onKKJe5xH2w"
      },
      "source": [
        "## Remove extra whitespaces\n",
        "Sometimes there are extra white spaces in the text which are necessary to be removed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ4LSnVLxokk",
        "outputId": "c1e3ec54-f348-4484-8ca7-2f32fbe4e10d"
      },
      "source": [
        "\n",
        "def remove_whitespace(text):\n",
        "    \"\"\"remove extra whitespaces from text\"\"\"\n",
        "    text = text.strip()  #removes any leading (spaces at the beginning) and trailing (spaces at the end) characters\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "print(remove_whitespace(\"   Here, there are extra    white     spaces  \"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here, there are extra white spaces\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDYIDqUtssQv"
      },
      "source": [
        "##Expand Contractions\n",
        "\n",
        "Contractions are shortened words, e.g., don’t and can’t. Expanding such words to “do not” and “can not” helps to standardize text.\n",
        "\n",
        "We use the contractions module to expand the contractions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUgnUTaqslVr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bfba0a46-7665-4441-8e20-a5edba7db5fd"
      },
      "source": [
        "import contractions\n",
        "\n",
        "def expand_contractions(text):\n",
        "    text = contractions.fix(text)\n",
        "    return text\n",
        "\n",
        "expand_contractions(\"\"\"expand shortened words, e.g. don't to do not\"\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'expand shortened words, e.g. do not to do not'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0SdoqcHs9ie"
      },
      "source": [
        "*Note: This step is optional depending on your NLP task as spaCy’s tokenization and lemmatization functions will perform the same effect to expand contractions such as can’t and don’t. The slight difference is that spaCy will expand “we’re” to “we be” while pycontractions will give result “we are”.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnUi04rktLLV"
      },
      "source": [
        "##Treatment for Numbers\n",
        "\n",
        "There are two steps in our treatment of numbers.\n",
        "\n",
        "One of the steps involve the conversion of number words to numeric form, e.g., seven to 7, to standardize text. To do this, we use the word2number module. Sample code as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wt7q4pBctR0f",
        "outputId": "5eeeca19-d1bd-4617-805e-ee182bad76eb"
      },
      "source": [
        "import spacy\n",
        "# load spacy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "from word2number import w2n\n",
        "\n",
        "text = \"\"\"three cups of coffee\"\"\"\n",
        "doc = nlp(text) #create a doc object\n",
        "tokens = [w2n.word_to_num(token.text) if token.pos_ == 'NUM' else token for token in doc]\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, cups, of, coffee]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svDGFXXCtN8b"
      },
      "source": [
        "\n",
        "The other step is to remove numbers. Removing numbers may make sense for sentiment analysis since numbers contain no information about sentiments. However, if our NLP task is to extract the number of tickets ordered in a message to our chatbot, we will definitely not want to remove numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HtgVnVFH3M3O",
        "outputId": "9debd9d6-31a2-4a95-85e3-d935055de50e"
      },
      "source": [
        "def remove_numbers(text):\n",
        "  text = re.sub(r\" \\d\", \"\", text)\n",
        "  return str(text)\n",
        "\n",
        "remove_numbers(\"Here, the is number 7 that we don't need.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Here, the is number that we don't need.\""
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvJVNu-Cs019"
      },
      "source": [
        "## II. Text Normalization\n",
        "\n",
        "Text normalization is the process of transforming a text into a canonical (standard) form. For example, the word “gooood” and “gud” can be transformed to “good”, its canonical form. Another example is mapping of near identical words such as “stopwords”, “stop-words” and “stop words” to just “stopwords”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqs9zQ2A0Z7g"
      },
      "source": [
        "1. Lowercase all texts\n",
        "2. Remove stopwords\n",
        "3. Lemmatization\n",
        "4. Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDiKlHw45bGi"
      },
      "source": [
        "## Lower case the text\n",
        "\n",
        "Converting all your data to lowercase helps in the process of preprocessing and in later stages in the NLP application, when you are doing parsing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gVQxKViQszq3",
        "outputId": "82a48abe-ecc5-49a0-ca3c-91672adc1a41"
      },
      "source": [
        "def convert_lower_case(text):\n",
        "    return str(np.char.lower(text))\n",
        "\n",
        "convert_lower_case(\"THIS is An ExaMple to Convert a Text To a lower CASE.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'this is an example to convert a text to a lower case.'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K41jif9R6IgP"
      },
      "source": [
        "## Remove Stop words\n",
        "\n",
        "Stopwords are referring to words that do not carry much insight, such as prepositions. NLTK and spaCy have different amounts of stopwords in the library, but both NLTK and spaCy allowed us to add in any word we feel necessary. For example, when we deal with email, we may add Gmail, com, outlook as stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iteW0Vgo6FSM",
        "outputId": "462376e5-8e0b-4d82-b534-bdb2653f30b0"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the list of stop words\n",
        "stop_words = stopwords.words('english')\n",
        "# add new stopwords to the list\n",
        "stop_words.extend([\"could\",\"though\",\"would\",\"also\",\"many\",'much'])\n",
        "print(stop_words)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'could', 'though', 'would', 'also', 'many', 'much']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgMhuf2S6hIU",
        "outputId": "1e1928d2-541f-4325-b598-9f738494cd0e"
      },
      "source": [
        "text = \"The idea of giving computers the ability to process human language is as old as the idea of computers themselves. \"\n",
        "\n",
        "# Remove the stopwords from the list of tokens\n",
        "tokens = [x for x in  word_tokenize(convert_lower_case(text)) if x not in stop_words]\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['idea', 'giving', 'computers', 'ability', 'process', 'human', 'language', 'old', 'idea', 'computers', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urIGnwnn8am-"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization is the process of converting a word to its base form, e.g., “caring” to “care”. We use spaCy’s lemmatizer to obtain the lemma, or base form, of the words. Sample code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfW4IhhG6w_6",
        "outputId": "1e44aa0e-71de-4041-e5de-09bba5d867ce"
      },
      "source": [
        "text = \"\"\"I'm happiness commitment running are\"\"\"\n",
        "doc = nlp(text) #create a doc object\n",
        "mytokens = [word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in doc] # -PRON- is the default lemma for pronouns in spaCy\n",
        "print(mytokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'be', 'happiness', 'commitment', 'run', 'be']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "Stemming is similar to lemmatization with the difference that in lemma word is actual word, but stem word can be a word without meaning."
      ],
      "metadata": {
        "id": "3hFU8FFx9l6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "def stemming(text):\n",
        "  token_words=word_tokenize(text)\n",
        "  stem_sentence=[]\n",
        "  for word in token_words:\n",
        "      stem_sentence.append(porter.stem(word))\n",
        "  return stem_sentence"
      ],
      "metadata": {
        "id": "GMTGIqgfGh0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemming(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nmzNqdcIgoE",
        "outputId": "624fb2bc-4aa1-40c4-bae0-be783aa1b73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', \"'m\", 'happi', 'commit', 'run', 'are']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming('happy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJzVCDcoJAe7",
        "outputId": "484a7fb9-3cda-42a7-e170-c5cea94bac08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['happi']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp1gnSW48w9J"
      },
      "source": [
        "##**Assignment:**\n",
        "\n",
        "Putting everything together, the full text preprocessing code (define a function as text_preprocessing):\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "09t57J-l9k7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"I'd like to have three cups   of coffee #?> Your Café is #awesome.\"\n",
        "\n",
        "def text_preprocessing(text):\n",
        "\n",
        "\n",
        "# result: I would like to have three cups of coffee? Your Cafe is awsome."
      ],
      "metadata": {
        "id": "tqAiYmrCc-Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxg22ZGTFXhi"
      },
      "source": [
        "Put all text normalization function together!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_normalization(text):\n",
        "\n",
        "\n",
        "#result: ['like', 'cup', 'coffee', 'cafe', 'delicious']"
      ],
      "metadata": {
        "id": "JVaGQr4sc7qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eXibEIg9Z56"
      },
      "source": [
        "Refrences:\n",
        "\n",
        "https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\n",
        "\n",
        "https://towardsdatascience.com/text-processing-in-python-29e86ea4114c"
      ]
    }
  ]
}